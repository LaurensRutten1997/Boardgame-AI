{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbf4311",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdfcf4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "parentdir = os.path.dirname(cwd)\n",
    "sys.path.insert(0, parentdir)\n",
    "\n",
    "from environments.tic_tac_toe.tic_tac_toe import TicTacToe\n",
    "from environments.game import PlayRoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27f2aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "playroom = PlayRoom(game = TicTacToe())\n",
    "\n",
    "class Transistion():\n",
    "  def __init__(self, state_code: str, action: int, reward: int, next_state_code: str):\n",
    "    self.state_code = state_code\n",
    "    self.action = action\n",
    "    self.next_state_code = next_state_code\n",
    "    self.reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7a59fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##EPSILON STILL NEEDED\n",
    "\n",
    "class QLearning():\n",
    "  def __init__(self, learning_rate: float, discount_factor: float, epsilon: float):\n",
    "    self.Q_table = {}\n",
    "    self.playroom = PlayRoom(game = TicTacToe())\n",
    "    self.epsilon = epsilon\n",
    "    self.learning_rate = learning_rate\n",
    "    self.discount_factor = discount_factor\n",
    "\n",
    "  def determine_discount_factor(self):\n",
    "    for df in range(0.1, 1.0, 0.1):\n",
    "      self.discount_factor = df\n",
    "      self.reset()\n",
    "      eval_x, eval_y = self.learn(n_epochs = 20_000, eval_freq = 1_000, num_eval_episodes = 500)\n",
    "      plt.show(eval_x, eval_y, label = f'{df}')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title('Peformance during training for different discount factors')\n",
    "    plt.ylabel('Trained epochs')\n",
    "    plt.xlabel('Average evaluation reward')\n",
    "    plt.show()\n",
    "    \n",
    "  def determine_epsilon(self):\n",
    "    for eps in range(0.1, 1.0, 0.1):\n",
    "      self.epsilon = eps\n",
    "      self.reset()\n",
    "      eval_x, eval_y = self.learn(n_epochs = 20_000, eval_freq = 1_000, num_eval_episodes = 500)\n",
    "      plt.show(eval_x, eval_y, label = f'{eps}')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title('Peformance during training for different epsilons')\n",
    "    plt.ylabel('Trained epochs')\n",
    "    plt.xlabel('Average evaluation reward')\n",
    "    plt.show()\n",
    "\n",
    "  def determine_learning_rate(self):\n",
    "    for lr in range(0.1, 1.0, 0.1):\n",
    "      self.learning_rate = lr\n",
    "      self.reset()\n",
    "      eval_x, eval_y = self.learn(n_epochs = 20_000, eval_freq = 1_000, num_eval_episodes = 500)\n",
    "      plt.show(eval_x, eval_y, label = f'{lr}')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title('Peformance during training for different learning rates')\n",
    "    plt.ylabel('Trained epochs')\n",
    "    plt.xlabel('Average evaluation reward')\n",
    "    plt.show()\n",
    "\n",
    "  def reset(self):\n",
    "    self.Q_table = {}\n",
    "\n",
    "  def learn(self, n_epochs: int, eval_freq: int, num_eval_episodes: int, early_stop: bool = True, early_stop_num: int = 10):\n",
    "    self.playroom.reset()\n",
    "    done = False\n",
    "\n",
    "    eval_y = []\n",
    "    eval_x = []\n",
    "    highest_eval = 0\n",
    "    not_improved = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "      action = self._determine_action(deterministic = False)\n",
    "      state_action_code = f'{self.playroom.game_state_code}-{action}'\n",
    "\n",
    "      if state_action_code not in self.Q_table.keys():\n",
    "        self.Q_table[state_action_code] = 0\n",
    "\n",
    "      _, reward, done, _, _ = self.playroom.step(action = action)\n",
    "      \n",
    "      if done:\n",
    "        self.playroom.reset()\n",
    "        self.Q_table[state_action_code] = (1 - self.learning_rate) * self.Q_table[state_action_code] + self.learning_rate * (reward)\n",
    "      \n",
    "      else:\n",
    "        next_optimal_state_action_code = f'{self.playroom.game_state_code}-{self._determine_action(deterministic = True)}'\n",
    "\n",
    "        if next_optimal_state_action_code not in self.Q_table.keys():\n",
    "          self.Q_table[next_optimal_state_action_code] = 0\n",
    "\n",
    "        self.Q_table[state_action_code] = (1 - self.learning_rate) * self.Q_table[state_action_code] + self.learning_rate * (reward + self.discount_factor * self.Q_table[next_optimal_state_action_code])\n",
    "\n",
    "      if (epoch + 1) % eval_freq == 0:\n",
    "        eval_y.append(self._evaluate(num_eval_episodes = num_eval_episodes))\n",
    "        eval_x.append(epoch + 1)\n",
    "\n",
    "        if highest_eval < eval_y[-1]:\n",
    "          highest_eval = eval_y[-1]\n",
    "          not_improved = 0\n",
    "        else:\n",
    "          not_improved += 1\n",
    "\n",
    "        print(f'Evaluate {eval_x[-1]}/{n_epochs}: {eval_y[-1]}')\n",
    "        self.playroom.reset()\n",
    "\n",
    "        if early_stop and not_improved > early_stop_num:\n",
    "          break\n",
    "    \n",
    "    plt.plot(eval_x, eval_y)\n",
    "    plt.title('Average reward during evalation')\n",
    "    plt.xlabel(\"Trained number of epochs\")\n",
    "    plt.ylabel('Average reward')\n",
    "    plt.show()\n",
    "\n",
    "    return eval_x, eval_y\n",
    "\n",
    "\n",
    "  def _determine_action(self, deterministic):\n",
    "    if random.random() < self.epsilon and not deterministic:\n",
    "      return random.choice(self.playroom.possible_actions)\n",
    "    \n",
    "    state_code = self.playroom.game_state_code\n",
    "    state_q_values = {k: v for k, v in self.Q_table.items() if state_code in k}\n",
    "\n",
    "    if len(state_q_values) == 0:\n",
    "      return random.choice(self.playroom.possible_actions)\n",
    "    \n",
    "    max_state_action = max(state_q_values, key = state_q_values.get)\n",
    "    return int(max_state_action.split('-')[-1])\n",
    "  \n",
    "  def _evaluate(self, num_eval_episodes: int):\n",
    "    total_reward = 0\n",
    "    for _ in range(num_eval_episodes):\n",
    "      self.playroom.reset()\n",
    "      done = False\n",
    "      while not done:\n",
    "        _, reward, done, _, _ = self.playroom.step(action = self._determine_action(deterministic = True))\n",
    "      total_reward += reward\n",
    "    return total_reward / num_eval_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d771313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate 1000/200000: -0.184\n",
      "Evaluate 2000/200000: -0.084\n",
      "Evaluate 3000/200000: -0.236\n",
      "Evaluate 4000/200000: -0.088\n",
      "Evaluate 5000/200000: -0.132\n",
      "Evaluate 6000/200000: -0.024\n",
      "Evaluate 7000/200000: -0.084\n",
      "Evaluate 8000/200000: 0.064\n",
      "Evaluate 9000/200000: 0.148\n",
      "Evaluate 10000/200000: 0.02\n",
      "Evaluate 11000/200000: 0.052\n",
      "Evaluate 12000/200000: -0.056\n",
      "Evaluate 13000/200000: 0.072\n",
      "Evaluate 14000/200000: 0.12\n",
      "Evaluate 15000/200000: 0.144\n",
      "Evaluate 16000/200000: 0.248\n",
      "Evaluate 17000/200000: 0.256\n",
      "Evaluate 18000/200000: 0.216\n",
      "Evaluate 19000/200000: 0.284\n",
      "Evaluate 20000/200000: 0.176\n",
      "Evaluate 21000/200000: 0.3\n",
      "Evaluate 22000/200000: 0.22\n",
      "Evaluate 23000/200000: 0.22\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m q_learing \u001b[38;5;241m=\u001b[39m QLearning(learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m, discount_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m, epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mq_learing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_eval_episodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[38], line 84\u001b[0m, in \u001b[0;36mQLearning.learn\u001b[1;34m(self, n_epochs, eval_freq, num_eval_episodes, early_stop, early_stop_num)\u001b[0m\n\u001b[0;32m     81\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_table[state_action_code] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_table[state_action_code] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m (reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscount_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_table[next_optimal_state_action_code])\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m eval_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 84\u001b[0m   eval_y\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_eval_episodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_eval_episodes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     85\u001b[0m   eval_x\u001b[38;5;241m.\u001b[39mappend(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m highest_eval \u001b[38;5;241m<\u001b[39m eval_y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "Cell \u001b[1;32mIn[38], line 127\u001b[0m, in \u001b[0;36mQLearning._evaluate\u001b[1;34m(self, num_eval_episodes)\u001b[0m\n\u001b[0;32m    125\u001b[0m   done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    126\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m--> 127\u001b[0m     _, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayroom\u001b[38;5;241m.\u001b[39mstep(action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_determine_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m    128\u001b[0m   total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_reward \u001b[38;5;241m/\u001b[39m num_eval_episodes\n",
      "Cell \u001b[1;32mIn[38], line 113\u001b[0m, in \u001b[0;36mQLearning._determine_action\u001b[1;34m(self, deterministic)\u001b[0m\n\u001b[0;32m    110\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayroom\u001b[38;5;241m.\u001b[39mpossible_actions)\n\u001b[0;32m    112\u001b[0m state_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayroom\u001b[38;5;241m.\u001b[39mgame_state_code\n\u001b[1;32m--> 113\u001b[0m state_q_values \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_table\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m state_code \u001b[38;5;129;01min\u001b[39;00m k}\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state_q_values) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    116\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayroom\u001b[38;5;241m.\u001b[39mpossible_actions)\n",
      "Cell \u001b[1;32mIn[38], line 113\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    110\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayroom\u001b[38;5;241m.\u001b[39mpossible_actions)\n\u001b[0;32m    112\u001b[0m state_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayroom\u001b[38;5;241m.\u001b[39mgame_state_code\n\u001b[1;32m--> 113\u001b[0m state_q_values \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_table\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mstate_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m}\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state_q_values) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    116\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayroom\u001b[38;5;241m.\u001b[39mpossible_actions)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q_learing = QLearning(learning_rate = 0.9, discount_factor = 0.9, epsilon = 0.5)\n",
    "q_learing.learn(200_000, eval_freq = 1_000, num_eval_episodes = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d7764c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
