{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "902cc949",
   "metadata": {},
   "source": [
    "## CartPole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be5cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# Instantiate the environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0902b92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Shape: (4,)\n",
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "#It's a vector with 4 components\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Shape:\", env.observation_space.shape)\n",
    "#There are two discrete actions\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d09466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The reset method is called at teh beginning of an episode\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af859fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: 1\n"
     ]
    }
   ],
   "source": [
    "#Sample a random action\n",
    "action = env.action_space.sample()\n",
    "print(f'Sampled action: {action}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc43202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step in the environment\n",
    "obs, reward, terminated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20d19dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,) 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "#Note the obs is a numpy array\n",
    "#info is an empty dict for now but can contain any debugging info\n",
    "#reward is a scalar\n",
    "print(obs.shape, reward, terminated, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ee3b6",
   "metadata": {},
   "source": [
    "### Write the function to collect data\n",
    "\n",
    "This function collects an offline dataset of transitions that will be used to train a model using the FQI algoritm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d560cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class OfflineData:\n",
    "  \"\"\"\n",
    "  A class to store transitions\n",
    "  \"\"\"\n",
    "  observations: np.ndarray #same as \"state\" in the theory\n",
    "  next_observations: np.ndarray\n",
    "  actions: np.ndarray\n",
    "  rewards: np.ndarray\n",
    "  terminateds: np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57c11ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(env_id: str, n_steps: int = 50_000) -> OfflineData:\n",
    "  \"\"\"\n",
    "  Collect transistions using a random agent (sample action randomly).\n",
    "\n",
    "  Args:\n",
    "      env_id (str): The name of the environment\n",
    "      n_steps (int, optional): Number of steps to perform in the env. Defaults to 50_000.\n",
    "\n",
    "  Returns:\n",
    "      OfflineData: The collected transitions\n",
    "  \"\"\"\n",
    "\n",
    "  # Create the Gym env\n",
    "  env = gym.make(env_id)\n",
    "\n",
    "  assert isinstance(env.observation_space, gym.spaces.Box)\n",
    "  # Numpy arrays (buffers) to collect the data\n",
    "  observations = np.zeros((n_steps, *env.observation_space.shape))\n",
    "  next_observations = np.zeros((n_steps, *env.observation_space.shape))\n",
    "  # Discrete actions\n",
    "  actions = np.zeros((n_steps, 1))\n",
    "  rewards = np.zeros((n_steps,))\n",
    "  terminateds = np.zeros((n_steps,))\n",
    "\n",
    "  obs = env.reset()\n",
    "\n",
    "  for idx in range(n_steps):\n",
    "    # Sample a random action\n",
    "    action = env.action_space.sample()\n",
    "    # Step in the environment\n",
    "    next_obs, reward, terminated, info = env.step(action)\n",
    "\n",
    "    # Store the transition\n",
    "    observations[idx, :] = obs\n",
    "    next_observations[idx, :] = next_obs\n",
    "    actions[idx, :] = action\n",
    "    rewards[idx] = reward\n",
    "    terminateds[idx] = terminated\n",
    "    # Update current observation\n",
    "    obs = next_obs\n",
    "\n",
    "    # Check if the episode is over\n",
    "    # Don't forget to reset the env at the end of an episode\n",
    "    if terminated:\n",
    "      obs = env.reset()\n",
    "\n",
    "  return OfflineData(\n",
    "    observations,\n",
    "    next_observations,\n",
    "    actions,\n",
    "    rewards,\n",
    "    terminateds\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1682cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "n_steps = 50_000\n",
    "data = collect_data(env_id = env_id, n_steps = n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2096bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the length of the collected data\n",
    "assert len(data.observations) == n_steps\n",
    "assert len(data.next_observations) == n_steps\n",
    "#Check that there are multiple episodes in the data\n",
    "assert not np.all(data.terminateds)\n",
    "assert np.any(data.terminateds)\n",
    "# Check the shape of the collected data\n",
    "if env_id == \"CartPole-v1\":\n",
    "    assert data.observations.shape == (n_steps, 4)\n",
    "    assert data.next_observations.shape == (n_steps, 4)\n",
    "assert data.actions.shape == (n_steps, 1)\n",
    "assert data.rewards.shape == (n_steps,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7358d28d",
   "metadata": {},
   "source": [
    "## Fitted Q Iteration (FQI) Algorithm\n",
    "\n",
    "Fitted Q Iteration (FQI) is an algorithm that extends q-learning to continuous state space using function estimators.\n",
    "\n",
    "It iteratively approximates the q-values for a given dataset of transitions (state, action, reward, next_state) by iteratively solving a regression problem.\n",
    "\n",
    "Compared to later algorithms like DQN, FQI uses the whole dataset at every iteration (working on the whole batch instead of using minibatches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f39e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from sklearn import tree\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed884d",
   "metadata": {},
   "source": [
    "### Choosing a Model\n",
    "\n",
    "With FQI, you can use any regression model.\n",
    "\n",
    "Here we are choosing a k-nearest neighbors regressor, but one could choose a linear model, decision tree, a neural network, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eaf59b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First choose the regressor\n",
    "model_class = partial(KNeighborsRegressor, n_neighbors = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b87ad3",
   "metadata": {},
   "source": [
    "### First Iteration of FQI\n",
    "\n",
    "For $n = 0$, the initial training set is defined as:\n",
    "\n",
    "- $x = (s_t, a_t)$\n",
    "- $y = r_t$\n",
    "\n",
    "We fit a regression model $f_\\theta(x) = y$ to obtain $ Q^{n=0}_\\theta(s, a) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29bcc52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First iteration:\n",
    "# The target q-value is the reward obtained\n",
    "targets = data.rewards.copy()\n",
    "# Create input for current observations and actions\n",
    "# Concatenate the observations and actions\n",
    "# so we can predict qf(s_t, a_t)\n",
    "current_obs_input = np.concatenate((data.observations, data.actions), axis = 1)\n",
    "model = model_class().fit(current_obs_input, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38cf95",
   "metadata": {},
   "source": [
    "### Write the function to predict Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3754caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_values(model: RegressorMixin, obs: np.ndarray, n_actions: int) -> np.ndarray:\n",
    "  \"\"\"Retrieve the q-values for a set of observations(= states in the theory).\n",
    "  qf(s_t, action) for all possible actions\n",
    "\n",
    "  Args:\n",
    "      model (RegressorMixin): Q_value estimator\n",
    "      obs (np.ndarray): A batch of observations\n",
    "      n_actions (int): Number of discrete actions\n",
    "\n",
    "  Returns:\n",
    "      np.ndarray: The predicted q-values for the given observations (batch_size, n_actions)\n",
    "  \"\"\"\n",
    "  \n",
    "  batch_size = len(obs)\n",
    "  q_values = np.zeros((batch_size, n_actions))\n",
    "\n",
    "  for action_idx in range(n_actions):\n",
    "    # Create a vector of size (batch_size, 1) for current action\n",
    "    actions = action_idx * np.ones((batch_size, 1))\n",
    "    # Concatentate the observations and the actions to obtain the input ot the q-value estimator\n",
    "    model_input = np.concatenate((obs, actions), axis = 1)\n",
    "    # Predict q-values for the given observation/action combination\n",
    "    predicted_q_values = model.predict(model_input)\n",
    "    # Update the q-values array for the current action\n",
    "    q_values[:, action_idx] = predicted_q_values\n",
    "  \n",
    "  return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5307642a",
   "metadata": {},
   "source": [
    "Let's test it with a subset of the collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cd6ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_observations = 2\n",
    "n_actions = int(env.action_space.n)\n",
    "\n",
    "q_values = get_q_values(model, data.observations[:n_observations], n_actions)\n",
    "\n",
    "assert q_values.shape == (n_observations, n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f710c777",
   "metadata": {},
   "source": [
    "### Write the function to evaluate a model\n",
    "\n",
    "A greedy policy $\\pi(s)$ can be defined using the q-value:\n",
    "\n",
    "$\\pi(s) = argmax_{a \\in A} Q(s, a)$.\n",
    "\n",
    "It is the policy that takes the action with the highest q-value for a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d4373fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "def evaluate(model: RegressorMixin, env: gym.Env, n_eval_episodes: int = 10, video_name: Optional[str] = None) -> None:\n",
    "  episode_returns = []\n",
    "  episode_reward = 0.0\n",
    "  total_episodes = 0\n",
    "  done = False\n",
    "\n",
    "  # Setup video recorder\n",
    "  if video_name is not None and env.render_mode == 'rgb_array':\n",
    "    os.makedirs(\"../logs/videos/\", exist_ok = True)\n",
    "\n",
    "    # New gym recorder always wants to cut video into episodes,\n",
    "    # set video length big enough but not to inf (will cut)\n",
    "    env = RecordVideo(env, \"../logs/videos\", step_trigger = lambda _: False, video_length = 100_000)\n",
    "    env.start_recording(video_name)\n",
    "  \n",
    "  current_obs = env.reset()\n",
    "  # Number of discrete actions\n",
    "  n_actions = int(env.action_space.n)\n",
    "  assert isinstance(env.action_space, spaces.Discrete), \"FQI only support discrete actions\"\n",
    "\n",
    "  while total_episodes < n_eval_episodes:\n",
    "    # Retrieve the q-values for the current observation\n",
    "    q_values = get_q_values(model = model, obs = np.expand_dims(current_obs, axis = 0), n_actions = n_actions)\n",
    "    # Select the action that maxmizes the q-value for each state\n",
    "    action = np.argmax(q_values)\n",
    "\n",
    "    current_obs, reward, terminated, info = env.step(action)\n",
    "\n",
    "    episode_reward += float(reward)\n",
    "\n",
    "    if terminated:\n",
    "      episode_returns.append(episode_reward)\n",
    "      episode_reward = 0.0\n",
    "      total_episodes += 1\n",
    "      current_obs = env.reset()\n",
    "\n",
    "  if isinstance(env, RecordVideo):\n",
    "    print(f\"Saving video to ../logs/videos/{video_name}\")\n",
    "    env.close()\n",
    "\n",
    "  print(f\"Total reward = {np.mean(episode_returns):.2f} +/- {np.std(episode_returns):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1469c999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 9.40 +/- 0.80\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, env, n_eval_episodes = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
