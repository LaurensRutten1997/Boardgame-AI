{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b23aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "616ad92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "device = torch.device(\n",
    "  \"cuda\" if torch.cuda.is_available() else\n",
    "  \"mps\" if torch.backends.mps.is_available() else\n",
    "  \"cpu\"\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58aa892",
   "metadata": {},
   "source": [
    "## Replay Memory\n",
    "\n",
    "Make a memory to keep track of all the transistions (state, action and next_state and reward relation). These transistions will be sampled at random to get training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb02ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "  def __init__(self, capacity):\n",
    "    self.memory = deque([], maxlen = capacity)\n",
    "\n",
    "  def push(self, *args):\n",
    "    \"\"\"Save a transition\"\"\"\n",
    "    self.memory.append(Transition(*args))\n",
    "  \n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.memory, batch_size)\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d70909d",
   "metadata": {},
   "source": [
    "## DQN algorithm\n",
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe28c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "  def __init__(self, n_observations, n_actions):\n",
    "    super(DQN, self).__init__()\n",
    "    self.layer1 = nn.Linear(n_observations, 128)\n",
    "    self.layer2 = nn.Linear(128, 128)\n",
    "    self.layer3 = nn.Linear(128, n_actions)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer1(x))\n",
    "    x = F.relu(self.layer2(x))\n",
    "    return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab97fb14",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "820a3a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128 #The number of transitions sampled from the replay buffer\n",
    "GAMMA = 0.99 #The discount factor for the rewards\n",
    "EPS_START = 0.9 #The starting value of epsilon (random action)\n",
    "EPS_END = 0.05 #The final value of epsilon\n",
    "EPS_DECAY = 1000 #Controls the rate of explonentional decay of epsilon, higher value means slower decay\n",
    "TAU = 0.005 #The update rate of the target network\n",
    "LR = 1e-4 #The learning rate of the ''Adam'' optimizer\n",
    "\n",
    "#Get the number of actions from the gym action_space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "#Get the number of state observations\n",
    "state = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr = LR, amsgrad = True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "  global steps_done\n",
    "  sample = random.random()\n",
    "  eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1 * steps_done / EPS_DECAY)\n",
    "  steps_done += 1\n",
    "  if sample > eps_threshold:\n",
    "    with torch.no_grad():\n",
    "      return policy_net(state).max(1).indices.view(1, 1)\n",
    "  else:\n",
    "    return torch.tensor([[env.action_space.sample()]], device = device, dtype = torch.long)\n",
    "  \n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations(show_result = False):\n",
    "  plt.figure(1)\n",
    "  durations_t = torch.tensor(episode_durations, dtype = torch.float)\n",
    "  if show_result:\n",
    "    plt.title('Result')\n",
    "  else:\n",
    "    plt.clf()\n",
    "    plt.title('Training')\n",
    "  plt.xlabel('Episode')\n",
    "  plt.ylabel('Duration')\n",
    "  plt.plot(durations_t.numpy())\n",
    "\n",
    "  if len(durations_t) >= 100:\n",
    "    means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "    means = torch.cat((torch.zeros(99), means))\n",
    "    plt.plot(means.numpy())\n",
    "  \n",
    "  plt.pause(0.001)\n",
    "  if is_ipython:\n",
    "    if not show_result:\n",
    "      display.display(plt.gcf())\n",
    "      display.clear_output(wait = True)\n",
    "    else:\n",
    "      display.display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f5cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "  if len(memory) < BATCH_SIZE:\n",
    "    return\n",
    "  transitions = memory.sample(BATCH_SIZE)\n",
    "  batch = Transition(*zip(*transitions))\n",
    "\n",
    "  non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device = device, dtype = torch.bool)\n",
    "  non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "  state_batch = torch.cat(batch.state)\n",
    "  action_batch = torch.cat(batch.action)\n",
    "  reward_batch = torch.cat(batch.reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
